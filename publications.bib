
@misc{hee_demystifying_2025,
	title = {Demystifying {Hateful} {Content}: {Leveraging} {Large} {Multimodal} {Models} for {Hateful} {Meme} {Detection} with {Explainable} {Decisions}},
	shorttitle = {Demystifying {Hateful} {Content}},
	url = {http://arxiv.org/abs/2502.11073},
	doi = {10.48550/arXiv.2502.11073},
	abstract = {Hateful meme detection presents a significant challenge as a multimodal task due to the complexity of interpreting implicit hate messages and contextual cues within memes. Previous approaches have fine-tuned pre-trained vision-language models (PT-VLMs), leveraging the knowledge they gained during pre-training and their attention mechanisms to understand meme content. However, the reliance of these models on implicit knowledge and complex attention mechanisms renders their decisions difficult to explain, which is crucial for building trust in meme classification. In this paper, we introduce IntMeme, a novel framework that leverages Large Multimodal Models (LMMs) for hateful meme classification with explainable decisions. IntMeme addresses the dual challenges of improving both accuracy and explainability in meme moderation. The framework uses LMMs to generate human-like, interpretive analyses of memes, providing deeper insights into multimodal content and context. Additionally, it uses independent encoding modules for both memes and their interpretations, which are then combined to enhance classification performance. Our approach addresses the opacity and misclassification issues associated with PT-VLMs, optimizing the use of LMMs for hateful meme detection. We demonstrate the effectiveness of IntMeme through comprehensive experiments across three datasets, showcasing its superiority over state-of-the-art models.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Hee, Ming Shan and Lee, Roy Ka-Wei},
	month = feb,
	year = {2025},
	note = {arXiv:2502.11073 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Preprint. Accepted at ICWSM'25},
	file = {Preprint PDF:files/470/Hee and Lee - 2025 - Demystifying Hateful Content Leveraging Large Multimodal Models for Hateful Meme Detection with Exp.pdf:application/pdf;Snapshot:files/471/2502.html:text/html},
}

@inproceedings{hee_recent_2024,
	address = {Miami, Florida, USA},
	title = {Recent {Advances} in {Online} {Hate} {Speech} {Moderation}: {Multimodality} and the {Role} of {Large} {Models}},
	shorttitle = {Recent {Advances} in {Online} {Hate} {Speech} {Moderation}},
	url = {https://aclanthology.org/2024.findings-emnlp.254/},
	doi = {10.18653/v1/2024.findings-emnlp.254},
	abstract = {Moderating hate speech (HS) in the evolving online landscape is a complex challenge, compounded by the multimodal nature of digital content. This survey examines recent advancements in HS moderation, focusing on the burgeoning role of large language models (LLMs) and large multimodal models (LMMs) in detecting, explaining, debiasing, and countering HS. We begin with a comprehensive analysis of current literature, uncovering how text, images, and audio interact to spread HS. The combination of these modalities adds complexity and subtlety to HS dissemination. We also identified research gaps, particularly in underrepresented languages and cultures, and highlight the need for solutions in low-resource settings. The survey concludes with future research directions, including novel AI methodologies, ethical AI governance, and the development of context-aware systems. This overview aims to inspire further research and foster collaboration towards responsible and human-centric approaches to HS moderation in the digital age.},
	urldate = {2025-03-03},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Hee, Ming Shan and Sharma, Shivam and Cao, Rui and Nandi, Palash and Nakov, Preslav and Chakraborty, Tanmoy and Lee, Roy Ka-Wei},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {4407--4419},
	file = {Full Text PDF:files/473/Hee et al. - 2024 - Recent Advances in Online Hate Speech Moderation Multimodality and the Role of Large Models.pdf:application/pdf},
}

@inproceedings{hee_bridging_2024,
	address = {Miami, Florida, USA},
	title = {Bridging {Modalities}: {Enhancing} {Cross}-{Modality} {Hate} {Speech} {Detection} with {Few}-{Shot} {In}-{Context} {Learning}},
	shorttitle = {Bridging {Modalities}},
	url = {https://aclanthology.org/2024.emnlp-main.445/},
	doi = {10.18653/v1/2024.emnlp-main.445},
	abstract = {The widespread presence of hate speech on the internet, including formats such as text-based tweets and multimodal memes, poses a significant challenge to digital platform safety. Recent research has developed detection models tailored to specific modalities; however, there is a notable gap in transferring detection capabilities across different formats. This study conducts extensive experiments using few-shot in-context learning with large language models to explore the transferability of hate speech detection between modalities. Our findings demonstrate that text-based hate speech examples can significantly enhance the classification accuracy of vision-language hate speech. Moreover, text-based demonstrations outperform vision-language demonstrations in few-shot learning settings. These results highlight the effectiveness of cross-modality knowledge transfer and offer valuable insights for improving hate speech detection systems.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 2024 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Hee, Ming Shan and Kumaresan, Aditi and Lee, Roy Ka-Wei},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {7785--7799},
	file = {Full Text PDF:files/475/Hee et al. - 2024 - Bridging Modalities Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learnin.pdf:application/pdf},
}

@misc{wu_longgenbench_2025,
	title = {{LongGenBench}: {Benchmarking} {Long}-{Form} {Generation} in {Long} {Context} {LLMs}},
	shorttitle = {{LongGenBench}},
	url = {http://arxiv.org/abs/2409.02076},
	doi = {10.48550/arXiv.2409.02076},
	abstract = {Current benchmarks like Needle-in-a-Haystack (NIAH), Ruler, and Needlebench focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences - a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce LongGenBench, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, LongGenBench evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on Ruler, all models struggled with long text generation on LongGenBench, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei},
	month = jan,
	year = {2025},
	note = {arXiv:2409.02076 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2025; Github: https://github.com/mozhu621/LongGenBench/},
	file = {Preprint PDF:files/478/Wu et al. - 2025 - LongGenBench Benchmarking Long-Form Generation in Long Context LLMs.pdf:application/pdf;Snapshot:files/479/2409.html:text/html},
}

@inproceedings{ng_sghatecheck_2024,
	address = {Mexico City, Mexico},
	title = {{SGHateCheck}: {Functional} {Tests} for {Detecting} {Hate} {Speech} in {Low}-{Resource} {Languages} of {Singapore}},
	shorttitle = {{SGHateCheck}},
	url = {https://aclanthology.org/2024.woah-1.24/},
	doi = {10.18653/v1/2024.woah-1.24},
	abstract = {To address the limitations of current hate speech detection models, we introduce SGHateCheck, a novel framework designed for the linguistic and cultural context of Singapore and Southeast Asia. It extends the functional testing approach of HateCheck and MHC, employing large language models for translation and paraphrasing into Singapore`s main languages, and refining these with native annotators. SGHateCheck reveals critical flaws in state-of-the-art models, highlighting their inadequacy in sensitive content moderation. This work aims to foster the development of more effective hate speech detection tools for diverse linguistic environments, particularly for Singapore and Southeast Asia contexts.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 8th {Workshop} on {Online} {Abuse} and {Harms} ({WOAH} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Ng, Ri Chi and Prakash, Nirmalendu and Hee, Ming Shan and Choo, Kenny Tsu Wei and Lee, Roy Ka-wei},
	editor = {Chung, Yi-Ling and Talat, Zeerak and Nozza, Debora and Plaza-del-Arco, Flor Miriam and RÃ¶ttger, Paul and Mostafazadeh Davani, Aida and Calabrese, Agostina},
	month = jun,
	year = {2024},
	pages = {312--327},
	file = {Full Text PDF:files/481/Ng et al. - 2024 - SGHateCheck Functional Tests for Detecting Hate Speech in Low-Resource Languages of Singapore.pdf:application/pdf},
}

@inproceedings{prakash_promptmtopic_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {{PromptMTopic}: {Unsupervised} {Multimodal} {Topic} {Modeling} of {Memes} using {Large} {Language} {Models}},
	isbn = {9798400701085},
	shorttitle = {{PromptMTopic}},
	url = {https://dl.acm.org/doi/10.1145/3581783.3613836},
	doi = {10.1145/3581783.3613836},
	abstract = {The proliferation of social media has given rise to a new form of communication: memes. Memes are multimodal and often contain a combination of text and visual elements that convey meaning, humor, and cultural significance. While meme analysis has been an active area of research, little work has been done on unsupervised multimodal topic modeling of memes, which is important for content moderation, social media analysis, and cultural studies. We propose PromptMTopic, a novel multimodal prompt-based model designed to learn topics from both text and visual modalities by leveraging the language modeling capabilities of large language models. Our model effectively extracts and clusters topics learned from memes, considering the semantic interaction between the text and visual modalities. We evaluate our proposed model through extensive experiments on three real-world meme datasets, which demonstrate its superiority over state-of-the-art topic modeling baselines in learning descriptive topics in memes. Additionally, our qualitative analysis shows that PromptMTopic can identify meaningful and culturally relevant topics from memes. Our work contributes to the understanding of the topics and themes of memes, a crucial form of communication in today's society. Disclaimer: This paper contains sensitive content that may be disturbing to some readers.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Prakash, Nirmalendu and Wang, Han and Hoang, Nguyen Khoi and Hee, Ming Shan and Lee, Roy Ka-Wei},
	month = oct,
	year = {2023},
	pages = {621--631},
	file = {Full Text PDF:files/483/Prakash et al. - 2023 - PromptMTopic Unsupervised Multimodal Topic Modeling of Memes using Large Language Models.pdf:application/pdf},
}

@inproceedings{cao_pro-cap_2023,
	address = {New York, NY, USA},
	series = {{MM} '23},
	title = {Pro-{Cap}: {Leveraging} a {Frozen} {Vision}-{Language} {Model} for {Hateful} {Meme} {Detection}},
	isbn = {9798400701085},
	shorttitle = {Pro-{Cap}},
	url = {https://dl.acm.org/doi/10.1145/3581783.3612498},
	doi = {10.1145/3581783.3612498},
	abstract = {Hateful meme detection is a challenging multimodal task that requires comprehension of both vision and language, as well as cross-modal interactions. Recent studies have tried to fine-tune pre-trained vision-language models (PVLMs) for this task. However, with increasing model sizes, it becomes important to leverage powerful PVLMs more efficiently, rather than simply fine-tuning them. Recently, researchers have attempted to convert meme images into textual captions and prompt language models for predictions. This approach has shown good performance but suffers from non-informative image captions. Considering the two factors mentioned above, we propose a probing-based captioning approach to leverage PVLMs in a zero-shot visual question answering (VQA) manner. Specifically, we prompt a frozen PVLM by asking hateful content-related questions and use the answers as image captions (which we call Pro-Cap), so that the captions contain information critical for hateful content detection. The good performance of models with Pro-Cap on three benchmarks validates the effectiveness and generalization of the proposed method1.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 31st {ACM} {International} {Conference} on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Cao, Rui and Hee, Ming Shan and Kuek, Adriel and Chong, Wen-Haw and Lee, Roy Ka-Wei and Jiang, Jing},
	month = oct,
	year = {2023},
	pages = {5244--5252},
	file = {Full Text PDF:files/485/Cao et al. - 2023 - Pro-Cap Leveraging a Frozen Vision-Language Model for Hateful Meme Detection.pdf:application/pdf},
}

@inproceedings{prakash_totaldefmeme_2023,
	address = {New York, NY, USA},
	series = {{MMSys} '23},
	title = {{TotalDefMeme}: {A} {Multi}-{Attribute} {Meme} dataset on {Total} {Defence} in {Singapore}},
	isbn = {9798400701481},
	shorttitle = {{TotalDefMeme}},
	url = {https://dl.acm.org/doi/10.1145/3587819.3592545},
	doi = {10.1145/3587819.3592545},
	abstract = {Total Defence is a defence policy combining and extending the concept of military defence and civil defence. While several countries have adopted total defence as their defence policy, very few studies have investigated its effectiveness. With the rapid proliferation of social media and digitalisation, many social studies have been focused on investigating policy effectiveness through specially curated surveys and questionnaires either through digital media or traditional forms. However, such references may not truly reflect the underlying sentiments about the target policies or initiatives of interest. People are more likely to express their sentiment using communication mediums such as starting topic thread on forums or sharing memes on social media. Using Singapore as a case reference, this study aims to address this research gap by proposing TotalDefMeme, a large-scale multi-modal and multi-attribute meme dataset that captures public sentiments toward Singapore's Total Defence policy. Besides supporting social informatics and public policy analysis of the Total Defence policy, TotalDefMeme can also support many downstream multi-modal machine learning tasks, such as aspect-based stance classification and multi-modal meme clustering. We perform baseline machine learning experiments on TotalDefMeme and evaluate its technical validity, and present possible future interdisciplinary research directions and application scenarios using the dataset as a baseline.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the 14th {ACM} {Multimedia} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Prakash, Nirmalendu and Hee, Ming Shan and Lee, Roy Ka-Wei},
	month = jun,
	year = {2023},
	pages = {369--375},
	file = {Full Text PDF:files/487/Prakash et al. - 2023 - TotalDefMeme A Multi-Attribute Meme dataset on Total Defence in Singapore.pdf:application/pdf},
}

@misc{hee_decoding_2023,
	title = {Decoding the {Underlying} {Meaning} of {Multimodal} {Hateful} {Memes}},
	url = {http://arxiv.org/abs/2305.17678},
	doi = {10.48550/arXiv.2305.17678},
	abstract = {Recent studies have proposed models that yielded promising performance for the hateful meme classification task. Nevertheless, these proposed models do not generate interpretable explanations that uncover the underlying meaning and support the classification output. A major reason for the lack of explainable hateful meme methods is the absence of a hateful meme dataset that contains ground truth explanations for benchmarking or training. Intuitively, having such explanations can educate and assist content moderators in interpreting and removing flagged hateful memes. This paper address this research gap by introducing Hateful meme with Reasons Dataset (HatReD), which is a new multimodal hateful meme dataset annotated with the underlying hateful contextual reasons. We also define a new conditional generation task that aims to automatically generate underlying reasons to explain hateful memes and establish the baseline performance of state-of-the-art pre-trained language models on this task. We further demonstrate the usefulness of HatReD by analyzing the challenges of the new conditional generation task in explaining memes in seen and unseen domains. The dataset and benchmark models are made available here: https://github.com/Social-AI-Studio/HatRed},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Hee, Ming Shan and Chong, Wen-Haw and Lee, Roy Ka-Wei},
	month = jun,
	year = {2023},
	note = {arXiv:2305.17678 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 9 pages. Accepted by IJCAI 2023},
	file = {Preprint PDF:files/490/Hee et al. - 2023 - Decoding the Underlying Meaning of Multimodal Hateful Memes.pdf:application/pdf;Snapshot:files/491/2305.html:text/html},
}

@misc{wang_evaluating_2023,
	title = {Evaluating {GPT}-3 {Generated} {Explanations} for {Hateful} {Content} {Moderation}},
	url = {http://arxiv.org/abs/2305.17680},
	doi = {10.48550/arXiv.2305.17680},
	abstract = {Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {Wang, Han and Hee, Ming Shan and Awal, Md Rabiul and Choo, Kenny Tsu Wei and Lee, Roy Ka-Wei},
	month = aug,
	year = {2023},
	note = {arXiv:2305.17680 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 9 pages, 2 figures, Accepted by International Joint Conference on Artificial Intelligence(IJCAI)},
	file = {Preprint PDF:files/494/Wang et al. - 2023 - Evaluating GPT-3 Generated Explanations for Hateful Content Moderation.pdf:application/pdf;Snapshot:files/495/2305.html:text/html},
}

@inproceedings{hee_explaining_2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {On {Explaining} {Multimodal} {Hateful} {Meme} {Detection} {Models}},
	isbn = {978-1-4503-9096-5},
	url = {https://dl.acm.org/doi/10.1145/3485447.3512260},
	doi = {10.1145/3485447.3512260},
	abstract = {Hateful meme detection is a new multimodal task that has gained significant traction in academic and industry research communities. Recently, researchers have applied pre-trained visual-linguistic models to perform the multimodal classification task, and some of these solutions have yielded promising results. However, what these visual-linguistic models learn for the hateful meme classification task remains unclear. For instance, it is unclear if these models are able to capture the derogatory or slurs references in multimodality (i.e., image and text) of the hateful memes. To fill this research gap, this paper propose three research questions to improve our understanding of these visual-linguistic models performing the hateful meme classification task. We found that the image modality contributes more to the hateful meme classification task, and the visual-linguistic models are able to perform visual-text slurs grounding to a certain extent. Our error analysis also shows that the visual-linguistic models have acquired biases, which resulted in false-positive predictions.},
	urldate = {2025-03-03},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Hee, Ming Shan and Lee, Roy Ka-Wei and Chong, Wen-Haw},
	month = apr,
	year = {2022},
	pages = {3651--3655},
	file = {Full Text PDF:files/497/Hee et al. - 2022 - On Explaining Multimodal Hateful Meme Detection Models.pdf:application/pdf},
}
